{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8931f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PBIN_DIR=/share_6/work/solepro-moriya/jupyter102/venv-3.9.10-cikm/bin\n",
      "/share_6/work/solepro-moriya/jupyter102/venv-3.9.10-cikm/bin:/root/.pyenv/versions/3.9.10/bin:/root/.pyenv/libexec:/root/.pyenv/plugins/python-build/bin:/root/.pyenv/shims:/root/.pyenv/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import sys, os\n",
    "pbin_dir = os.path.dirname(sys.executable)\n",
    "%env PBIN_DIR={pbin_dir}\n",
    "os.environ['PATH'] = f'{os.environ[\"PBIN_DIR\"]}:{os.environ[\"PATH\"]}'\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851bcd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'need', 'some', 'advice', 'on', 'where', 'to', 'go', 'on', 'vacation', ',', 'have', 'you', 'been', 'anywhere', 'lately', '?']\n"
     ]
    }
   ],
   "source": [
    "#!${PBIN_DIR}/pip install pydash\n",
    "import fileinput, json, copy, pydash, random\n",
    "from tqdm.auto import tqdm\n",
    "def randomize_cands(cands,true_index):\n",
    "    inds = list(range(len(cands)))\n",
    "    inds=random.sample(inds,len(inds))\n",
    "    cands=[cands[i] for i in inds]\n",
    "    label=inds.index(true_index)\n",
    "    return [cands,label]\n",
    "def add_response_cands(all_uttrs,cands,count,exclude_responses):\n",
    "    l=len(cands)+count\n",
    "    while True:\n",
    "        if len(cands) == l:\n",
    "            break\n",
    "        u = random.choice(all_uttrs)\n",
    "        if u in cands:\n",
    "            continue\n",
    "        if u in exclude_responses:\n",
    "            continue\n",
    "        cands.append(u)\n",
    "    return cands\n",
    "def get_tsv_groups(div,grouplen):\n",
    "    with open(f'/share_7/projects/hais/data/msc_bertfp/last_history/{div}.tsv',encoding='utf-8') as f:\n",
    "        lines=[x.strip().split('\\t') for x in f.read().strip().split('\\n')]\n",
    "    groups=[]\n",
    "    group = None\n",
    "    for line in lines:\n",
    "        if line[0] == '1':\n",
    "            if group is not None:\n",
    "                assert(len(group)==grouplen)\n",
    "                groups.append(group)\n",
    "            group = []\n",
    "        session,author=line[1].split(' ',maxsplit=1)\n",
    "        group.append([session,author,line[-1].strip()])\n",
    "    assert(len(group)==grouplen)\n",
    "    groups.append(group)\n",
    "    return groups\n",
    "def create_data(div,tsvdiv,tsvgrouplen,respcount,outfile):\n",
    "    convs = {}\n",
    "    prev_count = 0\n",
    "    settings=[\n",
    "        [f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_2/{div}.txt', True],\n",
    "        [f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_3/{div}.txt', False],\n",
    "        [f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_4/{div}.txt', False]\n",
    "    ]\n",
    "    if div != 'train':\n",
    "        settings.append([f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_5/{div}.txt',False])\n",
    "    all_uttrs=[]\n",
    "    if True:\n",
    "        for inp, prev in settings:\n",
    "            fi = fileinput.FileInput((inp,))\n",
    "            for line in fi:\n",
    "                d = json.loads(line)\n",
    "                ses=d['dialog'][0]['convai2_id']\n",
    "                if ' ' in ses:\n",
    "                    assert(False)\n",
    "                if prev:\n",
    "                    t = [[x['text'].strip(),None] for x in d['previous_dialogs'][0]['dialog']] \n",
    "                    convs[ses] = {\n",
    "                        'u':t\n",
    "                    }\n",
    "                else:\n",
    "                    if 'i' not in convs[ses]:\n",
    "                        #index0のidがおかしい場合があるので避けて取る\n",
    "                        convs[ses]['i']=[x['id'] for x in d['dialog']][2:4]\n",
    "                        #if convs[ses]['i'][0]=='56152':\n",
    "                        #    print(inp)\n",
    "                        #    print(ses)\n",
    "                        #    print(d['dialog'][0])\n",
    "                        #    print(convs[ses])\n",
    "                        convs[ses]['p']=d['personas']\n",
    "                convs[ses]['u'] += [[x['text'].strip(),x['id']] for x in d['dialog']]\n",
    "    for ses in convs:\n",
    "        all_uttrs+=[x[0] for x in convs[ses]['u']]\n",
    "        all_uttrs+=pydash.flatten(convs[ses]['p'])\n",
    "    all_uttrs=pydash.uniq(all_uttrs)\n",
    "    return all_uttrs\n",
    "all_uttrs=create_data('train','',10,10,'a')\n",
    "#!${PBIN_DIR}/pip install gensim\n",
    "import json\n",
    "import pydash\n",
    "from torchtext.data import get_tokenizer\n",
    "from gensim.models import word2vec\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "wordss = [tokenizer(sentence) for sentence in all_uttrs]\n",
    "#padlen = 50\n",
    "#for i, words in enumerate(wordss):\n",
    "#    wordss[i] = ['<PAD>'] * (padlen - len(words)) + words\n",
    "print(wordss[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578207c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "#https://radimrehurek.com/gensim/models/word2vec.html#\n",
    "#https://rare-technologies.com/word2vec-tutorial/\n",
    "model = word2vec.Word2Vec(wordss,vector_size=200,workers=multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c365a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12527\n"
     ]
    }
   ],
   "source": [
    "wv = model.wv\n",
    "vocab = {'<PAD>':0}\n",
    "#PADは本家でも0になってる\n",
    "embeddings = [[0.0]*200]\n",
    "print(len(wv))\n",
    "for i in range(len(wv)):\n",
    "    vocab[wv.index_to_key[i]] = i+1\n",
    "    #normalizeして出力する方法もあるがcikmのembeddingみるとしてなさそう\n",
    "    embeddings.append(wv[i])\n",
    "import pickle\n",
    "with open(\"vocab_and_embeddings_msc.pkl\", mode=\"wb\") as f:\n",
    "    pickle.dump([vocab,embeddings], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28182b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.9.10-cikm2",
   "language": "python",
   "name": "venv-3.9.10-cikm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
