{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8931f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import sys, os\n",
    "pbin_dir = os.path.dirname(sys.executable)\n",
    "%env PBIN_DIR={pbin_dir}\n",
    "os.environ['PATH'] = f'{os.environ[\"PBIN_DIR\"]}:{os.environ[\"PATH\"]}'\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96922558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#オリジナル\n",
    "#%%capture output\n",
    "#output.show()で見れる or ファイルが出力されているか実際に確認する。\n",
    "\n",
    "import pydash\n",
    "import pickle\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"dataset/reddit/vocab_and_embeddings.pkl\", 'rb'))\n",
    "#cemb\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_redpol.pkl\", 'rb'))\n",
    "vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_nfl_minill3.pkl\", 'rb'))\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_politics_minill3.pkl\", 'rb'))\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "#cemb\n",
    "unkkey=list(vocab.keys())[-1]\n",
    "#unkkey='unk'\n",
    "print(f'unkkey: {unkkey}')\n",
    "unkv = vocab[unkkey]\n",
    "print(f'unkv: {unkv}')\n",
    "import json\n",
    "import random\n",
    "def conv_uttr(pe,seqlen,vocab):\n",
    "    ts = [tokenizer(x) for x in pe]\n",
    "    if any([len(x) > seqlen for x in ts]):\n",
    "        return None\n",
    "    #以下3行はmin3,min3top2000の計測の際は入れていた\n",
    "    #if any(['(' in x for x in ts]):\n",
    "    #    continue\n",
    "    #ts = [[y.replace('\\\\n', '').strip() for y in x] for x in ts]\n",
    "    #ts = [[y for y in x if y != ''] for x in ts]\n",
    "    ts = [['<PAD>'] * (seqlen - len(x)) + x for x in ts]\n",
    "    ts = [[vocab.get(y, vocab[unkkey]) for y in x] for x in ts]\n",
    "    return ts\n",
    "def create_test_dict(tsv,atsv):\n",
    "    tsvd = []\n",
    "    atsvd = []\n",
    "    ret = {}\n",
    "    with open(tsv) as f:\n",
    "        tsvd = [x.split('\\t') for x in f.read().strip().splitlines()]\n",
    "    with open(atsv) as f:\n",
    "        atsvd = [x.split('\\t') for x in f.read().strip().splitlines()]\n",
    "    for d, a in zip(tsvd,atsvd):\n",
    "        ca = a[-2]\n",
    "        if ca not in ret:\n",
    "            ret[ca] = {'p':[],'n':[]}\n",
    "        ret[ca]['p' if d[0] == '1' else 'n'].append([d[1:],a])\n",
    "    for retk in ret:\n",
    "        retp = ret[retk]['p']\n",
    "        retn = ret[retk]['n']\n",
    "        assert(len(retp) == 1)\n",
    "        assert(len(retn) == 9)\n",
    "        for retne in retn:\n",
    "            if retp[0][0][-1] == retne[0][-1]:\n",
    "                print(retp[0])\n",
    "                print(retne)\n",
    "                #politicsの場合おなじresponseのものがtestデータに1件ある\n",
    "                #assert(False)\n",
    "    return ret\n",
    "\n",
    "def create_data(d,u,pklname,tsv=None,atsv=None):\n",
    "    #以下はmin3,min3top2000のときは50としていた\n",
    "    seqlen = 50\n",
    "    ud = []\n",
    "    rdt = []\n",
    "    yd = []\n",
    "    debugd = []\n",
    "    unkc = 0\n",
    "    tc = 0\n",
    "    er1 = 0\n",
    "    er2 = 0\n",
    "    er3 = 0\n",
    "    keys = d.keys()\n",
    "    if u is not None:\n",
    "        keys = u\n",
    "    print(f'keyc: {len(keys)}')\n",
    "    print(keys[0])#author\n",
    "    \n",
    "    for k in keys:\n",
    "        hs = d[k]#authorの履歴\n",
    "        #min3,min3top2000の計測の際は以下を入れてた\n",
    "        ude = []\n",
    "        #min3,min3top2000の計測の際は通常のresponse authorをcontext authorとみなしてた\n",
    "        ohs = hs#なんのために変数をわざわざ変える?\n",
    "        #print(len(hs))\n",
    "        #print(len(ohs))\n",
    "        oh = ohs[-1]#authorの最新履歴\n",
    "#         print(oh)\n",
    "#         print()\n",
    "        ps = []\n",
    "        roh = oh['response_author']#authorの最新履歴のレスポンス側のauthor\n",
    "          \n",
    "        if roh in d:#jsonのauthorキーにresponse_authorがいたら、\n",
    "            hs = d[roh]#response_authorの履歴リストを取得\n",
    "            #hs = hs[:3]#debug\n",
    "            hs.sort(key=lambda x: int(x['context_created_utc']))#contextの時刻でソート\n",
    "            #uniqhist\n",
    "            hs = pydash.uniq_by(hs,lambda x: x['Context'])\n",
    "            for hi, h in enumerate(hs):#取得したresponse_authorの履歴から取得したデータをすべてpsにappendする\n",
    "                #print(hi)#history index\n",
    "                #print(h)#履歴\n",
    "                \n",
    "                if h['context_created_utc'] >= oh['response_created_utc']:#response_authorの履歴のcontext時間が、authorの最新履歴のcontext時間以上(新しい)場合はcontinue?\n",
    "                    continue\n",
    "                if 'context/0' not in h:#response_authorの履歴にcontext/0がない場合continue\n",
    "                    continue\n",
    "                ps.append([h[x] for x in ['context/0','Context']])#response_authorの履歴のcontext/0とContextをpsにappend\n",
    "        ps.append([oh[x] for x in ['Context', 'Response']])#authorの履歴のContextとResponseをpsにappend\n",
    "#         print(len(ps))\n",
    "#         print()\n",
    "        \n",
    "#         print(ps)\n",
    "#         print(dsa)\n",
    "        er2o=False\n",
    "        \n",
    "        for pi, pe in enumerate(ps):#上記処理で取得したresponse_author履歴+authorの最新履歴のリスト\n",
    "            #print(pe)\n",
    "            ts = conv_uttr(pe,seqlen,vocab)#id化、(先頭indexから0埋めされている)\n",
    "            ###　例\n",
    "            #[\n",
    "            #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 459, 40, 401, 443, 26, 35, 283],\n",
    "            #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 208, 12, 3, 18, 11125, 19, 10494, 2, 221, 10, 438, 58, 8, 822]\n",
    "            #]\n",
    "            ###\n",
    "            #print(ts)\n",
    "            \n",
    "            if ts is None:#conv_utterの結果がnoneのときのスキップフラグオン？\n",
    "                if pi == len(ps) - 1:\n",
    "                    er2+=1\n",
    "                    er2o=True\n",
    "                continue\n",
    "                \n",
    "            for t in ts:\n",
    "                unkc += len([x for x in t if x == unkv])#unknownIDがあれば加算？\n",
    "                tc += len(t)#seqの長さを足す(合計100になるはず)\n",
    "                ude.append(t)\n",
    "            \n",
    "        if er2o:\n",
    "            continue\n",
    "            \n",
    "        if len(ude) == 0:\n",
    "            er3 +=1\n",
    "            continue\n",
    "        \n",
    "#         print(ude)\n",
    "#         print()\n",
    "        resp = ude[-1]#response側？\n",
    "#         print(resp)\n",
    "        ude = ude[-30:-1]#最新から29個分？\n",
    "#         print(ude)\n",
    "        ude = [*([[0] * seqlen] * (29 - len(ude))), *ude]\n",
    "\n",
    "        \n",
    "        for i in range(10):\n",
    "            debugd.append([k,roh])\n",
    "            if i == 0:\n",
    "                yd.append(1)\n",
    "                #デバッグ用にrawレスポンスとresponse authorを仕込む\n",
    "                rdt.append([resp,k,ps[-1][1],roh])\n",
    "            else:\n",
    "                yd.append(0)\n",
    "            ud.append(ude)#加工した29データを10個分用意している\n",
    "        \n",
    " \n",
    "        #1authorの履歴からのデータ処理はここまで\n",
    "            \n",
    "        ### udについて\n",
    "        #[ud,rd,yd]がpickleになってる\n",
    "        #ydは0 or 1のarray\n",
    "        #rdは当てに行く返答。n個に1個正解（yd=1）がある\n",
    "        \n",
    "        #udはrdを当てるためのデータ。長さ29のarrayで中身は発話で、最後の要素以外はだれか（だれかは毎回異なって良い）の発話とそれに対するrdの返答者による返答となる。\n",
    "        #最後の要素はだれかの発話となり、rdの返答はこの最後のだれかの発話への返答となる。長さが余る場合は後ろから埋めて、あまりを空発話にする\n",
    "        #返答・発話は、長さが固定（seqlen）のarrayで中身はtoken id(integer)。余る場合は先頭からに対応するidで埋める。空発話であれば、全てに対応するidとなる\n",
    "        #\n",
    "            \n",
    "    test_dict=None\n",
    "    if tsv is not None:\n",
    "        test_dict=create_test_dict(tsv,atsv)\n",
    "    rd = []\n",
    "    for rdi, rde in enumerate(rdt):\n",
    "        rd.append(rde[0])\n",
    "        if test_dict is None:\n",
    "            for i in range(9):\n",
    "                sr = list(range(len(rdt)))\n",
    "                sr.remove(rdi)\n",
    "                si = random.choice(sr)\n",
    "                rd.append(rdt[si][0])\n",
    "        else:\n",
    "            test_dict_ent = test_dict[rde[1]]\n",
    "            tdep = test_dict_ent['p'][0]\n",
    "            tdepd,tdepa=tdep\n",
    "            assert(tdepa[-2]==rde[1])\n",
    "            assert(tdepa[-1]==rde[3])\n",
    "            try:\n",
    "                assert(all([x==y for x,y in zip(conv_uttr([tdepd[-1]],seqlen,vocab)[0],rde[0])]))\n",
    "            except:\n",
    "                print(len(rd))\n",
    "                print(tdepd)\n",
    "                print(tdepa)\n",
    "                print(conv_uttr([tdepd[-1]],seqlen,vocab)[0])\n",
    "                print(rde)\n",
    "                assert(False)\n",
    "            assert(yd[rdi*10] == 1)\n",
    "            for tdei, tde in enumerate(test_dict_ent['n']):\n",
    "                assert(len(rd)%10==tdei+1)\n",
    "                assert(yd[rdi*10+tdei+1] == 0)\n",
    "                d,a=tde\n",
    "                assert(a[-2]==rde[1])\n",
    "                ts = conv_uttr([d[-1]],seqlen,vocab)\n",
    "                assert(ts is not None)\n",
    "                rd.append(ts[0])\n",
    "            #frd=rd[-10]\n",
    "            #for rrdi, rrd in enumerate(rd[-9:]):\n",
    "                #unkによって1wordのresponseが同じにみなされることはある\n",
    "                #if all([x == y for x,y in zip(rrd,frd)]):\n",
    "                #    print(rde[1])\n",
    "                #    print(rde[2])\n",
    "                #    print(test_dict_ent['n'][rrdi][0][-1])\n",
    "                #    print(rrdi)\n",
    "                #    print(frd)\n",
    "                #    print(rrd)\n",
    "                #    assert(False)\n",
    "    print(f'unkc: {unkc}')\n",
    "    print(f'tc: {tc}')\n",
    "    print(f'unkc/tc: {unkc/tc}')\n",
    "    print(f'{er1},{er2},{er3}')\n",
    "    assert(er1==0)\n",
    "    #politicsの場合trainデータにあるっぽい\n",
    "    #assert(er2==0)\n",
    "    assert(er3==0)\n",
    "    assert(len(ud)==len(rd))\n",
    "    assert(len(ud)==len(yd))\n",
    "    with open(pklname,'wb') as f:\n",
    "        pickle.dump([ud,rd,yd],f)\n",
    "    with open(f'{pklname}_debug.pkl','wb') as f:\n",
    "        pickle.dump(debugd,f)\n",
    "    a,b,c = pickle.load(file=open(pklname, 'rb'))\n",
    "\n",
    "d = None\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_test.json') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_test.json') as f:\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_test.json') as f:\n",
    "    d = json.loads(f.read())\n",
    "    \n",
    "u = None\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_politics_minill3_test_top10000.txt') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_nfl_minill3_test_top3000.txt') as f:\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/author_list_min30_201809101112_20190102_politics_minill3_test.txt') as f:\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_politics_minill3_test.txt') as f:\n",
    "    u = f.read().strip().split('\\n')\n",
    "#create_data(d, u,\n",
    "#            'tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl',\n",
    "#            '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test.tsv',\n",
    "#            '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test_authors.tsv')\n",
    "\n",
    "#create_data(d, u,\n",
    "#           'tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl',\n",
    "#           '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_test.tsv',\n",
    "#           '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_test_authors.tsv')\n",
    "\n",
    "# create_data(d, u,\n",
    "#            'tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl',\n",
    "#            '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test.tsv',\n",
    "#            '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test_authors.tsv')\n",
    "\n",
    "\n",
    "#create_data(d, u, 'tm_test_min20_clean_bugfix_try.pkl')\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_train.json') as f:\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_train.json') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_train.json') as f:\n",
    "    d = json.loads(f.read())\n",
    "#u = None #もともと\n",
    "\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_politics_minill3_train_top10000.txt') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_11503_201809101112_20190102_nfl_minill3_train_top500.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/train_reddit_politics_top3000_ctxloop50_resloop50.txt') as f:\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/train_reddit_nlf_top3000_ctxloop50_resloop50.txt') as f:\n",
    "\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_1983_201809101112_20190102_nfl_minill3_train.txt') as f:\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/author_list_min30_9619_201809101112_20190102_politics_minill3_train.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_14501_201809101112_20190102_politics_minill3_train.txt') as f:\n",
    "    u = f.read().strip().split('\\n')\n",
    "    \n",
    "create_data(d, u, 'tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# create_data(d, u, \n",
    "#             'tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl',\n",
    "#            '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_train.tsv',\n",
    "#            '/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_train_authors.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%capture output\n",
    "#ループカウントとスライド実装版\n",
    "#output.show()で見れる or ファイルが出力されているか実際に確認する。\n",
    "\n",
    "import pydash\n",
    "import pickle\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"dataset/reddit/vocab_and_embeddings.pkl\", 'rb'))\n",
    "#cemb\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_redpol.pkl\", 'rb'))\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_nfl_minill3.pkl\", 'rb'))\n",
    "vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_politics_minill3.pkl\", 'rb'))\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "#cemb\n",
    "unkkey=list(vocab.keys())[-1]\n",
    "#unkkey='unk'\n",
    "print(f'unkkey: {unkkey}')\n",
    "unkv = vocab[unkkey]\n",
    "print(f'unkv: {unkv}')\n",
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def conv_uttr(pe,seqlen,vocab):\n",
    "    ts = [tokenizer(x) for x in pe]\n",
    "    if any([len(x) > seqlen for x in ts]):\n",
    "        return None\n",
    "    #以下3行はmin3,min3top2000の計測の際は入れていた\n",
    "    #if any(['(' in x for x in ts]):\n",
    "    #    continue\n",
    "    #ts = [[y.replace('\\\\n', '').strip() for y in x] for x in ts]\n",
    "    #ts = [[y for y in x if y != ''] for x in ts]\n",
    "    ts = [['<PAD>'] * (seqlen - len(x)) + x for x in ts]\n",
    "    ts = [[vocab.get(y, vocab[unkkey]) for y in x] for x in ts]\n",
    "    return ts\n",
    "def create_test_dict(tsv,atsv):\n",
    "    tsvd = []\n",
    "    atsvd = []\n",
    "    ret = {}\n",
    "    with open(tsv) as f:\n",
    "        tsvd = [x.split('\\t') for x in f.read().strip().splitlines()]\n",
    "    with open(atsv) as f:\n",
    "        atsvd = [x.split('\\t') for x in f.read().strip().splitlines()]\n",
    "    for d, a in zip(tsvd,atsvd):\n",
    "        ca = a[-2]\n",
    "        if ca not in ret:\n",
    "            ret[ca] = {'p':[],'n':[]}\n",
    "        ret[ca]['p' if d[0] == '1' else 'n'].append([d[1:],a])\n",
    "    for retk in ret:\n",
    "        retp = ret[retk]['p']\n",
    "        retn = ret[retk]['n']\n",
    "        assert(len(retp) == 1)\n",
    "        assert(len(retn) == 9)\n",
    "        for retne in retn:\n",
    "            if retp[0][0][-1] == retne[0][-1]:\n",
    "                print(retp[0])\n",
    "                print(retne)\n",
    "                #politicsの場合おなじresponseのものがtestデータに1件ある\n",
    "                #assert(False)\n",
    "    return ret\n",
    "\n",
    "debug_author=[]\n",
    "debug_response_author=[]\n",
    "def create_data(d,u,pklname,tsv=None,atsv=None,loop_count=1,max_slide_num=1):\n",
    "    global debug_author\n",
    "    global debug_response_author\n",
    "    #以下はmin3,min3top2000のときは50としていた\n",
    "    seqlen = 50\n",
    "    ud = []\n",
    "    rdt = []\n",
    "    yd = []\n",
    "    debugd = []\n",
    "    unkc = 0\n",
    "    tc = 0\n",
    "    er1 = 0\n",
    "    er2 = 0\n",
    "    er3 = 0\n",
    "    keys = d.keys()\n",
    "    if u is not None:\n",
    "        keys = u\n",
    "    print(f'keyc: {len(keys)}')#author\n",
    "    \n",
    "    for k in tqdm(keys,desc=\"create_data\"):\n",
    "        hs = d[k]#authorの履歴\n",
    "        for start in range(max_slide_num):\n",
    "            num_histories = len(hs)\n",
    "            histories = hs[\n",
    "            max([num_histories - loop_count - start, 0]):max([num_histories - start, 0])\n",
    "                                           ]#最新の履歴から見てloopcount分スライドしながら取り出す\n",
    "            #print([x for x in range(max([num_histories - loop_count - start, 0]),max([num_histories - start, 0]))])\n",
    "            #print(len([x for x in range(max([num_histories - loop_count - start, 0]),max([num_histories - start, 0]))]))\n",
    "            num_histories = len(histories)\n",
    "            if num_histories != loop_count:\n",
    "                # 取り出した履歴データがloopcount分に満たない場合はスキップ\n",
    "                #print(\"skip\")\n",
    "                continue\n",
    "            #histories = hs[-loop_count:]#loop_count分の履歴を対象にデータ処理\n",
    "            for history in histories:\n",
    "                #min3,min3top2000の計測の際は以下を入れてた\n",
    "                ude = []\n",
    "                #min3,min3top2000の計測の際は通常のresponse authorをcontext authorとみなしてた\n",
    "                #ohs = hs#なんのために変数をわざわざ変える?\n",
    "                #oh = ohs[-1]#authorの最新履歴\n",
    "                oh = history\n",
    "                ps = []\n",
    "                roh = oh['response_author']#historyのレスポンス側のauthor\n",
    "\n",
    "\n",
    "                if roh in d:#jsonのauthorキーにresponse_authorがいたら、\n",
    "                    hs = d[roh]#response_authorの履歴リストを取得\n",
    "                    #hs = hs[:3]#debug\n",
    "                    hs.sort(key=lambda x: int(x['context_created_utc']))#contextの時刻でソート\n",
    "                    #uniqhist\n",
    "                    hs = pydash.uniq_by(hs,lambda x: x['Context'])\n",
    "                    for hi, h in enumerate(hs):#取得したresponse_authorの履歴から取得したデータをすべてpsにappendする\n",
    "                        #print(hi)#history index\n",
    "                        #print(h)#履歴\n",
    "\n",
    "                        if h['context_created_utc'] >= oh['response_created_utc']:#response_authorの履歴のcontext時間が、authorの最新履歴のcontext時間以上(新しい)場合はcontinue?\n",
    "                            continue\n",
    "                        if 'context/0' not in h:#response_authorの履歴にcontext/0がない場合continue\n",
    "                            continue\n",
    "                        ps.append([h[x] for x in ['context/0','Context']])#response_authorの履歴のcontext/0とContextをpsにappend 誰かの発話とrdの返答(Context)\n",
    "                ps.append([oh[x] for x in ['Context', 'Response']])#authorの履歴のContextとResponseをpsにappend\n",
    "\n",
    "                er2o=False\n",
    "\n",
    "                for pi, pe in enumerate(ps):#上記処理で取得したresponse_author履歴+authorの最新履歴のリスト\n",
    "                    ts = conv_uttr(pe,seqlen,vocab)#id化、(先頭indexから0埋めされている?)\n",
    "                    ###　例\n",
    "                    #[\n",
    "                    #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 459, 40, 401, 443, 26, 35, 283],\n",
    "                    #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 208, 12, 3, 18, 11125, 19, 10494, 2, 221, 10, 438, 58, 8, 822]\n",
    "                    #]\n",
    "                    ###\n",
    "                    #print(ts)\n",
    "\n",
    "                    if ts is None:#conv_utterの結果がnoneのときのスキップフラグオン？\n",
    "                        if pi == len(ps) - 1:\n",
    "                            er2+=1\n",
    "                            er2o=True\n",
    "                        continue\n",
    "\n",
    "                    for t in ts:\n",
    "                        unkc += len([x for x in t if x == unkv])#unknownIDがあれば加算？\n",
    "                        tc += len(t)#seqの長さを足す(合計100になるはず)\n",
    "                        ude.append(t)\n",
    "\n",
    "                if er2o:\n",
    "                    continue\n",
    "\n",
    "                if len(ude) == 0:\n",
    "                    er3 +=1\n",
    "                    continue\n",
    "\n",
    "                resp = ude[-1]#response側？\n",
    "                ude = ude[-30:-1]#反転させているが、なぜ-30から-1なのか不明\n",
    "                ude = [*([[0] * seqlen] * (29 - len(ude))), *ude]\n",
    "\n",
    "\n",
    "                for i in range(10):\n",
    "                    debugd.append([k,roh])\n",
    "                    if i == 0:\n",
    "                        yd.append(1)#ラベル\n",
    "                        #デバッグ用にrawレスポンスとresponse authorを仕込む\n",
    "                        rdt.append([resp,k,ps[-1][1],roh])#正解データ\n",
    "                    else:\n",
    "                        yd.append(0)\n",
    "                    ud.append(ude)#加工した29データを10個分用意している\n",
    "                    \n",
    "                debug_author.append(k)\n",
    "                debug_response_author.append(roh)\n",
    "                \n",
    "    print(len(ud))\n",
    "    print(len(yd))\n",
    "    print(len(rdt))\n",
    "    \n",
    "    test_dict=None\n",
    "    if tsv is not None:\n",
    "        test_dict=create_test_dict(tsv,atsv)\n",
    "    rd = []\n",
    "    for rdi, rde in tqdm(enumerate(rdt),desc=\"rdt\"):\n",
    "        rd.append(rde[0])\n",
    "        if test_dict is None:\n",
    "            #train\n",
    "            #不正解データ9個\n",
    "            for i in range(9):\n",
    "                sr = list(range(len(rdt)))\n",
    "                sr.remove(rdi)\n",
    "                si = random.choice(sr)\n",
    "                rd.append(rdt[si][0])\n",
    "        else:\n",
    "            #test valid\n",
    "            test_dict_ent = test_dict[rde[1]]\n",
    "            tdep = test_dict_ent['p'][0]\n",
    "            tdepd,tdepa=tdep\n",
    "            assert(tdepa[-2]==rde[1])\n",
    "            assert(tdepa[-1]==rde[3])\n",
    "            try:\n",
    "                assert(all([x==y for x,y in zip(conv_uttr([tdepd[-1]],seqlen,vocab)[0],rde[0])]))\n",
    "            except:\n",
    "                print(len(rd))\n",
    "                print(tdepd)\n",
    "                print(tdepa)\n",
    "                print(conv_uttr([tdepd[-1]],seqlen,vocab)[0])\n",
    "                print(rde)\n",
    "                assert(False)\n",
    "            assert(yd[rdi*10] == 1)\n",
    "            for tdei, tde in enumerate(test_dict_ent['n']):\n",
    "                assert(len(rd)%10==tdei+1)\n",
    "                assert(yd[rdi*10+tdei+1] == 0)\n",
    "                d,a=tde\n",
    "                assert(a[-2]==rde[1])\n",
    "                ts = conv_uttr([d[-1]],seqlen,vocab)\n",
    "                assert(ts is not None)\n",
    "                rd.append(ts[0])\n",
    "\n",
    "    print(f'unkc: {unkc}')\n",
    "    print(f'tc: {tc}')\n",
    "    print(f'unkc/tc: {unkc/tc}')\n",
    "    print(f\"ud:{len(ud)}\")\n",
    "    print(f\"rd:{len(rd)}\")\n",
    "    print(f\"yd:{len(yd)}\")\n",
    "    print(f'{er1},{er2},{er3}')\n",
    "    print(f\"author:{len(list(set(debug_author)))}\")\n",
    "    print(f\"response_author:{len(list(set(debug_response_author)))}\")\n",
    "\n",
    "\n",
    "    assert(er1==0)\n",
    "    #politicsの場合trainデータにあるっぽい\n",
    "    #assert(er2==0)\n",
    "    assert(er3==0)\n",
    "    assert(len(ud)==len(rd))\n",
    "    assert(len(ud)==len(yd))\n",
    "    with open(pklname,'wb') as f:\n",
    "        pickle.dump([ud,rd,yd],f)\n",
    "    with open(f'{pklname}_debug.pkl','wb') as f:\n",
    "        pickle.dump(debugd,f)\n",
    "    print(\"pkl saved\")\n",
    "    a,b,c = pickle.load(file=open(pklname, 'rb'))\n",
    "    print(\"done\")\n",
    "\n",
    "#test\n",
    "\n",
    "\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_test.json') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_test.json') as f:\n",
    "    d = json.loads(f.read())\n",
    "    \n",
    "# with open(\"/share_7/projects/hais/data/reddit_nfl/author_list_min20_13765_201809101112_20190102_nfl_minill3_test_top500.txt\") as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_nfl_minill3_test.txt') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_politics_minill3_test.txt') as f:\n",
    "     u = f.read().strip().split('\\n')\n",
    "\n",
    "create_data(d,u,\n",
    "            \"tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl\",\n",
    "            \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test.tsv\",\n",
    "            \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test_authors.tsv\",\n",
    "            1,1\n",
    "           )\n",
    "# create_data(d,u,\n",
    "#             \"tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl\",\n",
    "#             \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_test.tsv\",\n",
    "#             \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_test_authors.tsv\",\n",
    "#             1,1\n",
    "#            )\n",
    "\n",
    "\n",
    "#train\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_train.json') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_train.json') as f:\n",
    "    d = json.loads(f.read())\n",
    "\n",
    "# with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_11503_201809101112_20190102_nfl_minill3_train_top500.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_1983_201809101112_20190102_nfl_minill3_train.txt') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_14501_201809101112_20190102_politics_minill3_train.txt') as f:\n",
    "    u = f.read().strip().split('\\n')\n",
    "    \n",
    "\n",
    "create_data(d, u, \n",
    "            'tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl',\n",
    "            None,\n",
    "            None,\n",
    "            3,20)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%capture output\n",
    "#slide分の履歴を対象に19件のペルソナ版\n",
    "\n",
    "import pydash\n",
    "import pickle\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"dataset/reddit/vocab_and_embeddings.pkl\", 'rb'))\n",
    "#cemb\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_redpol.pkl\", 'rb'))\n",
    "vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_nfl_minill3.pkl\", 'rb'))\n",
    "#vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_politics_minill3.pkl\", 'rb'))\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "#cemb\n",
    "unkkey=list(vocab.keys())[-1]\n",
    "#unkkey='unk'\n",
    "print(f'unkkey: {unkkey}')\n",
    "unkv = vocab[unkkey]\n",
    "print(f'unkv: {unkv}')\n",
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def conv_uttr(pe,seqlen,vocab):\n",
    "    ts = [tokenizer(x) for x in pe]\n",
    "    if any([len(x) > seqlen for x in ts]):\n",
    "        return None\n",
    "    #以下3行はmin3,min3top2000の計測の際は入れていた\n",
    "    #if any(['(' in x for x in ts]):\n",
    "    #    continue\n",
    "    #ts = [[y.replace('\\\\n', '').strip() for y in x] for x in ts]\n",
    "    #ts = [[y for y in x if y != ''] for x in ts]\n",
    "    ts = [['<PAD>'] * (seqlen - len(x)) + x for x in ts]\n",
    "    ts = [[vocab.get(y, vocab[unkkey]) for y in x] for x in ts]\n",
    "    return ts\n",
    "def create_test_dict(tsv,atsv):\n",
    "    tsvd = []\n",
    "    atsvd = []\n",
    "    ret = {}\n",
    "    with open(tsv) as f:\n",
    "        tsvd = [x.split('\\t') for x in f.read().strip().splitlines()]\n",
    "    with open(atsv) as f:\n",
    "        atsvd = [x.split('\\t') for x in f.read().strip().splitlines()]\n",
    "    for d, a in zip(tsvd,atsvd):\n",
    "        ca = a[-2]\n",
    "        if ca not in ret:\n",
    "            ret[ca] = {'p':[],'n':[]}\n",
    "        ret[ca]['p' if d[0] == '1' else 'n'].append([d[1:],a])\n",
    "    for retk in ret:\n",
    "        retp = ret[retk]['p']\n",
    "        retn = ret[retk]['n']\n",
    "        assert(len(retp) == 1)\n",
    "        assert(len(retn) == 9)\n",
    "        for retne in retn:\n",
    "            if retp[0][0][-1] == retne[0][-1]:\n",
    "                print(retp[0])\n",
    "                print(retne)\n",
    "                #politicsの場合おなじresponseのものがtestデータに1件ある\n",
    "                #assert(False)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_data(d,u,pklname,tsv=None,atsv=None,loop_count=1):\n",
    "    debug_author=[]\n",
    "    debug_response_author=[]\n",
    "    #以下はmin3,min3top2000のときは50としていた\n",
    "    seqlen = 50\n",
    "    ud = []\n",
    "    rdt = []\n",
    "    yd = []\n",
    "    debugd = []\n",
    "    unkc = 0\n",
    "    tc = 0\n",
    "    er1 = 0\n",
    "    er2 = 0\n",
    "    er3 = 0\n",
    "    keys = d.keys()\n",
    "    \n",
    "    if u is not None:\n",
    "        keys = u\n",
    "    print(f'keyc: {len(keys)}')#author\n",
    "    \n",
    "    for k in tqdm(keys,desc=\"create_data\"):\n",
    "        hs = d[k]#authorの履歴\n",
    "        \n",
    "        histories = hs[-loop_count:]#loop_count分の履歴を対象にデータ処理\n",
    "        debug_author.append(k)\n",
    "        debug_response_author.append(histories[-1][\"response_author\"])\n",
    "        for history in histories:\n",
    "            ude = []\n",
    "            oh = history\n",
    "            ps = []\n",
    "            roh = oh['response_author']#historyのレスポンス側のauthor\n",
    "\n",
    "            if roh in d:#jsonのauthorキーにresponse_authorがいたら、\n",
    "                hs = d[roh]#response_authorの履歴リストを取得\n",
    "                #hs = hs[:3]#debug\n",
    "                hs.sort(key=lambda x: int(x['context_created_utc']))#contextの時刻でソート\n",
    "                #uniqhist\n",
    "                hs = pydash.uniq_by(hs,lambda x: x['Context'])\n",
    "                for hi, h in enumerate(hs):#取得したresponse_authorの履歴から取得したデータをすべてpsにappendする\n",
    "                    #print(hi)#history index\n",
    "                    #print(h)#履歴\n",
    "                    if h['context_created_utc'] >= oh['response_created_utc']:#response_authorの履歴のcontext時間が、authorの最新履歴のcontext時間以上(新しい)場合はcontinue?\n",
    "                        continue\n",
    "                    if 'context/0' not in h:#response_authorの履歴にcontext/0がない場合continue\n",
    "                        continue\n",
    "                    ps.append([h[x] for x in ['context/0','Context']])#response_authorの履歴のcontext/0とContextをpsにappend 誰かの発話とrdの返答(Context)\n",
    "            ps.append([oh[x] for x in ['Context', 'Response']])#authorの履歴のContextとResponseをpsにappend\n",
    "\n",
    "            er2o=False\n",
    "\n",
    "            for pi, pe in enumerate(ps):#上記処理で取得したresponse_author履歴+authorの最新履歴のリスト\n",
    "                ts = conv_uttr(pe,seqlen,vocab)#id化、(先頭indexから0埋めされている?)\n",
    "                ###　例\n",
    "                #[\n",
    "                #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 459, 40, 401, 443, 26, 35, 283],\n",
    "                #[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 208, 12, 3, 18, 11125, 19, 10494, 2, 221, 10, 438, 58, 8, 822]\n",
    "                #]\n",
    "                ###\n",
    "                #print(ts)\n",
    "\n",
    "                if ts is None:#conv_utterの結果がnoneのときのスキップフラグオン？\n",
    "                    if pi == len(ps) - 1:\n",
    "                        er2+=1\n",
    "                        er2o=True\n",
    "                    continue\n",
    "\n",
    "                for t in ts:\n",
    "                    unkc += len([x for x in t if x == unkv])#unknownIDがあれば加算？\n",
    "                    tc += len(t)#seqの長さを足す(合計100になるはず)\n",
    "                    ude.append(t)\n",
    "\n",
    "            if er2o:\n",
    "                continue\n",
    "\n",
    "            if len(ude) == 0:\n",
    "                er3 +=1\n",
    "                continue\n",
    "                \n",
    "            ud_num = 19 #rdを当てに行くデータの数 偶数だと学習できない\n",
    "            \n",
    "            resp = ude[-1]#response側？\n",
    "            #ude = ude[-30:-1]#-30から-1 = 29件のデータに絞る\n",
    "            #ude = [*([[0] * seqlen] * (29 - len(ude))), *ude]\n",
    "            ude = ude[-ud_num+1:-1]\n",
    "            ude = [*([[0] * seqlen] * (ud_num - len(ude))), *ude]\n",
    "\n",
    "            for i in range(10):\n",
    "                debugd.append([k,roh])\n",
    "                if i == 0:\n",
    "                    yd.append(1)#ラベル\n",
    "                    #デバッグ用にrawレスポンスとresponse authorを仕込む\n",
    "                    rdt.append([resp,k,ps[-1][1],roh])#正解データ\n",
    "                else:\n",
    "                    yd.append(0)\n",
    "                ud.append(ude)#加工した29件データを10個分用意している\n",
    "\n",
    "            \n",
    "                \n",
    "    print(len(ud))\n",
    "    print(len(yd))\n",
    "    print(len(rdt))\n",
    "    \n",
    "    test_dict=None\n",
    "    if tsv is not None:\n",
    "        test_dict=create_test_dict(tsv,atsv)\n",
    "    rd = []\n",
    "    for rdi, rde in tqdm(enumerate(rdt),desc=\"rdt\"):\n",
    "        rd.append(rde[0])\n",
    "        if test_dict is None:\n",
    "            #train\n",
    "            #不正解データ9個?\n",
    "            for i in range(9):\n",
    "                sr = list(range(len(rdt)))\n",
    "                sr.remove(rdi)\n",
    "                si = random.choice(sr)\n",
    "                rd.append(rdt[si][0])\n",
    "        else:\n",
    "            #test valid\n",
    "            test_dict_ent = test_dict[rde[1]]\n",
    "            tdep = test_dict_ent['p'][0]\n",
    "            tdepd,tdepa=tdep\n",
    "            assert(tdepa[-2]==rde[1])\n",
    "            assert(tdepa[-1]==rde[3])\n",
    "            try:\n",
    "                assert(all([x==y for x,y in zip(conv_uttr([tdepd[-1]],seqlen,vocab)[0],rde[0])]))\n",
    "            except:\n",
    "                print(len(rd))\n",
    "                print(tdepd)\n",
    "                print(tdepa)\n",
    "                print(conv_uttr([tdepd[-1]],seqlen,vocab)[0])\n",
    "                print(rde)\n",
    "                assert(False)\n",
    "            assert(yd[rdi*10] == 1)\n",
    "            for tdei, tde in enumerate(test_dict_ent['n']):\n",
    "                assert(len(rd)%10==tdei+1)\n",
    "                assert(yd[rdi*10+tdei+1] == 0)\n",
    "                d,a=tde\n",
    "                assert(a[-2]==rde[1])\n",
    "                ts = conv_uttr([d[-1]],seqlen,vocab)\n",
    "                assert(ts is not None)\n",
    "                rd.append(ts[0])\n",
    "\n",
    "    print(f'unkc: {unkc}')\n",
    "    print(f'tc: {tc}')\n",
    "    print(f'unkc/tc: {unkc/tc}')\n",
    "    print(f\"ud:{len(ud)}\")\n",
    "    print(f\"ud_num:{len(ud[0])}\")\n",
    "    print(f\"rd:{len(rd)}\")\n",
    "    print(f\"yd:{len(yd)}\")\n",
    "    print(f'{er1},{er2},{er3}')\n",
    "    print(f\"author:{len(list(set(debug_author)))}\")\n",
    "    print(f\"response_author:{len(list(set(debug_response_author)))}\")\n",
    "    \n",
    "\n",
    "    assert(er1==0)\n",
    "    #politicsの場合trainデータにあるっぽい\n",
    "    #assert(er2==0)\n",
    "    assert(er3==0)\n",
    "    assert(len(ud)==len(rd))\n",
    "    assert(len(ud)==len(yd))\n",
    "    with open(pklname,'wb') as f:\n",
    "        pickle.dump([ud,rd,yd],f)\n",
    "    with open(f'{pklname}_debug.pkl','wb') as f:\n",
    "        pickle.dump(debugd,f)\n",
    "    print(\"pkl saved\")\n",
    "    a,b,c = pickle.load(file=open(pklname, 'rb'))\n",
    "    print(\"done\")\n",
    "\n",
    "#test\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_test.json') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_test.json') as f:\n",
    "    d = json.loads(f.read())\n",
    "    \n",
    "#with open(\"/share_7/projects/hais/data/reddit_nfl/author_list_min20_13765_201809101112_20190102_nfl_minill3_test_top500.txt\") as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_nfl_minill3_test.txt') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_nfl_minill3_test_top3000.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_politics_minill3_test.txt') as f:\n",
    "#with open(\"/share_7/projects/hais/data/reddit_nfl/author_list_min20_19044_201809101112_20190102_politics_minill3_test_top500.txt\") as f:\n",
    "     u = f.read().strip().split('\\n')\n",
    "\n",
    "        \n",
    "# create_data(d,u,\n",
    "#             \"tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl\",\n",
    "#             \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test.tsv\",\n",
    "#             \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3politics_data/201809101112_20190102_politics_minill3_test_authors.tsv\",\n",
    "#             1\n",
    "#            )\n",
    "\n",
    "create_data(d,u,\n",
    "            \"tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl\",\n",
    "            \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_test.tsv\",\n",
    "            \"/share_7/projects/hais/data/reddit_nfl_bertfp/minill3nfl10_data/201809101112_20190102_nfl_minill3_test_authors.tsv\",\n",
    "            1\n",
    "           )\n",
    "\n",
    "\n",
    "#train\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_nfl_minill3_train.json') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/201809101112_20190102_politics_minill3_train.json') as f:\n",
    "    d = json.loads(f.read())\n",
    "\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_11503_201809101112_20190102_nfl_minill3_train_top500.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_1983_201809101112_20190102_nfl_minill3_train.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_201809101112_20190102_nfl_minill3_train.txt') as f:\n",
    "with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_1983_201809101112_20190102_nfl_minill3_train.txt') as f:\n",
    "#with open('/share_7/projects/hais/data/reddit_nfl/author_list_min20_14501_201809101112_20190102_politics_minill3_train.txt') as f:\n",
    "#with open(\"/share_7/projects/hais/data/reddit_nfl/author_list_min20_14501_201809101112_20190102_politics_minill3_train_top500.txt\") as f:\n",
    "    u = f.read().strip().split('\\n')\n",
    "    \n",
    "\n",
    "create_data(d, u, \n",
    "            'tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl',\n",
    "            None,\n",
    "            None,\n",
    "            10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "#nfl_minill3_min20_top3000\n",
    "#politics_minill3_min30\n",
    "#politics_minill3_min20\n",
    "\"\"\"\n",
    "!cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit.pkl\n",
    "!cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit.pkl\n",
    "!cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit.pkl\n",
    "!cp vocab_and_embeddings_redpol.pkl dataset/reddit/vocab_and_embeddings.pkl\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_nfl_minill3_min20_all.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_nfl_minill3_min20_all.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_nfl_minill3_min20_all.pkl\n",
    "# !cp vocab_and_embeddings_nfl_minill3.pkl dataset/reddit/vocab_and_embeddings_nfl_minill3_min20_all.pkl\n",
    "\n",
    "\n",
    "\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_politics_minill3_min30.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_politics_minill3_min30.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_politics_minill3_min30.pkl\n",
    "# !cp vocab_and_embeddings_politics_minill3.pkl dataset/reddit/vocab_and_embeddings_politics_minill3_min30.pkl\n",
    "\n",
    "\n",
    "\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_politics_minill3_min20.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_politics_minill3_min20.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_politics_minill3_min20.pkl\n",
    "# !cp vocab_and_embeddings_politics_minill3.pkl dataset/reddit/vocab_and_embeddings_politics_minill3_min20.pkl\n",
    "\n",
    "#slide10 history19 nfl min 20\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_nfl_minill3_min20_all_lc10.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_nfl_minill3_min20_all_lc10.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_nfl_minill3_min20_all_lc10.pkl\n",
    "# !cp vocab_and_embeddings_nfl_minill3.pkl dataset/reddit/vocab_and_embeddings_nfl_minill3_min20_all_lc10.pkl\n",
    "\n",
    "#slide10 history19 nfl min 20 top3000\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_nfl_minill3_min20_top3000_lc10.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_nfl_minill3_min20_top3000_lc10.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_nfl_minill3_min20_top3000_lc10.pkl\n",
    "# !cp vocab_and_embeddings_nfl_minill3.pkl dataset/reddit/vocab_and_embeddings_nfl_minill3_min20_top3000_lc10.pkl\n",
    "\n",
    "#slide10 history19 politics min20\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_politics_minill3_min20_all_lc10.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_politics_minill3_min20_all_lc10.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_politics_minill3_min20_all_lc10.pkl\n",
    "# !cp vocab_and_embeddings_politics_minill3.pkl dataset/reddit/vocab_and_embeddings_politics_minill3_min20_all_lc10.pkl\n",
    "\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/dev_reddit_politics_minill3_min20_top500_lc10.pkl\n",
    "# !cp tm_test_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/test_reddit_politics_minill3_min20_top500_lc10.pkl\n",
    "# !cp tm_train_redpol_min20_top10000_clean_bugfix_uniqhist_cemb.pkl dataset/reddit/train_reddit_politics_minill3_min20_top500_lc10.pkl\n",
    "# !cp vocab_and_embeddings_politics_minill3.pkl dataset/reddit/vocab_and_embeddings_politics_minill3_min20_top500_lc10.pkl\n",
    "\n",
    "!pwd\n",
    "print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f28cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "# !cp tm_test_min20_clean_bugfix_uniqhist.pkl dataset/reddit2/dev_reddit2.pkl\n",
    "# !cp tm_test_min20_clean_bugfix_uniqhist.pkl dataset/reddit2/test_reddit2.pkl\n",
    "# !cp tm_train_min20_old_clean_uniqhist.pkl dataset/reddit2/train_reddit2.pkl\n",
    "# print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40056e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1eda7",
   "metadata": {},
   "source": [
    "# データ構造解説\n",
    "\n",
    "- [ud,rd,yd]がpickleになってる\n",
    "- ydは0 or 1のarray\n",
    "- rdは当てに行く返答。n個に1個正解（yd=1）がある\n",
    "- udはrdを当てるためのデータ。長さ29のarrayで中身は発話で、最後の要素以外はだれか（だれかは毎回異なって良い）の発話とそれに対するrdの返答者による返答となる。最後の要素はだれかの発話となり、rdの返答はこの最後のだれかの発話への返答となる。長さが余る場合は後ろから埋めて、あまりを空発話にする\n",
    "- 返答・発話は、長さが固定（seqlen）のarrayで中身はtoken id(integer)。余る場合は先頭から<PAD>に対応するidで埋める。空発話であれば、全て<PAD>に対応するidとなる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.9.10-cikm2",
   "language": "python",
   "name": "venv-3.9.10-cikm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
