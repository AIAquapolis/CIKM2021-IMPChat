{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8931f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PBIN_DIR=/share_6/work/solepro-moriya/jupyter102/venv-3.9.10-cikm/bin\n",
      "/share_6/work/solepro-moriya/jupyter102/venv-3.9.10-cikm/bin:/root/.pyenv/versions/3.9.10/bin:/root/.pyenv/libexec:/root/.pyenv/plugins/python-build/bin:/root/.pyenv/shims:/root/.pyenv/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import sys, os\n",
    "pbin_dir = os.path.dirname(sys.executable)\n",
    "%env PBIN_DIR={pbin_dir}\n",
    "os.environ['PATH'] = f'{os.environ[\"PBIN_DIR\"]}:{os.environ[\"PATH\"]}'\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71e266e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unkkey: 41\n",
      "unkv: 12527\n",
      "avgconlen:30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_data: 100%|██████████| 7991/7991 [00:06<00:00, 1200.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er1:8,er2:0,er3:1,er4:0,er5:26672\n",
      "avgconlen:30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_data: 100%|██████████| 1002/1002 [00:00<00:00, 1132.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er1:0,er2:0,er3:0,er4:0,er5:5758\n"
     ]
    }
   ],
   "source": [
    "#!${PBIN_DIR}/pip install pydash\n",
    "import pydash\n",
    "import pickle\n",
    "vocab, word_embeddings = pickle.load(file=open(f\"vocab_and_embeddings_msc.pkl\", 'rb'))\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "unkkey=list(vocab.keys())[-1]\n",
    "print(f'unkkey: {unkkey}')\n",
    "unkv = vocab[unkkey]\n",
    "print(f'unkv: {unkv}')\n",
    "import fileinput, json, copy, random\n",
    "from tqdm.auto import tqdm\n",
    "def conv_uttr(x,seqlen,vocab,unkkey):\n",
    "    x = tokenizer(x)\n",
    "    seqlenover=False\n",
    "    if len(x)>seqlen:\n",
    "        seqlenover=True\n",
    "        x=x[:seqlen]\n",
    "    x = ['<PAD>'] * (seqlen - len(x)) + x\n",
    "    x = [vocab.get(y, vocab[unkkey]) for y in x]\n",
    "    return [x,seqlenover]\n",
    "def randomize_cands(cands,true_index):\n",
    "    inds = list(range(len(cands)))\n",
    "    inds=random.sample(inds,len(inds))\n",
    "    cands=[cands[i] for i in inds]\n",
    "    label=inds.index(true_index)\n",
    "    return [cands,label]\n",
    "def add_response_cands(all_uttrs,cands,count,exclude_responses):\n",
    "    l=len(cands)+count\n",
    "    while True:\n",
    "        if len(cands) == l:\n",
    "            break\n",
    "        u = random.choice(all_uttrs)\n",
    "        if u in cands:\n",
    "            continue\n",
    "        if u in exclude_responses:\n",
    "            continue\n",
    "        cands.append(u)\n",
    "    return cands\n",
    "def get_tsv_groups(div,grouplen):\n",
    "    with open(f'/share_7/projects/hais/data/msc_bertfp/last_history/{div}.tsv',encoding='utf-8') as f:\n",
    "        lines=[x.strip().split('\\t') for x in f.read().strip().split('\\n')]\n",
    "    groups=[]\n",
    "    group = None\n",
    "    for line in lines:\n",
    "        if line[0] == '1':\n",
    "            if group is not None:\n",
    "                assert(len(group)==grouplen)\n",
    "                groups.append(group)\n",
    "            group = []\n",
    "        session,author=line[1].split(' ',maxsplit=1)\n",
    "        group.append([session,author,line[-1].strip()])\n",
    "    assert(len(group)==grouplen)\n",
    "    groups.append(group)\n",
    "    return groups\n",
    "def create_data(div,tsvdiv,tsvgrouplen,respcount,seqlen,outfile):\n",
    "    convs = {}\n",
    "    prev_count = 0\n",
    "    settings=[\n",
    "        [f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_2/{div}.txt', True],\n",
    "        [f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_3/{div}.txt', False],\n",
    "        [f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_4/{div}.txt', False]\n",
    "    ]\n",
    "    if div != 'train':\n",
    "        settings.append([f'/share_6/work/solepro-moriya/jupyter102/selfplay/msc/msc_dialogue/session_5/{div}.txt',False])\n",
    "    all_uttrs=[]\n",
    "    if True:\n",
    "        for inp, prev in settings:\n",
    "            fi = fileinput.FileInput((inp,))\n",
    "            for line in fi:\n",
    "                d = json.loads(line)\n",
    "                ses=d['dialog'][0]['convai2_id']\n",
    "                if ' ' in ses:\n",
    "                    assert(False)\n",
    "                if prev:\n",
    "                    t = [[x['text'].strip(),None] for x in d['previous_dialogs'][0]['dialog']] \n",
    "                    convs[ses] = {\n",
    "                        'u':t\n",
    "                    }\n",
    "                else:\n",
    "                    if 'i' not in convs[ses]:\n",
    "                        #index0のidがおかしい場合があるので避けて取る\n",
    "                        convs[ses]['i']=[x['id'] for x in d['dialog']][2:4]\n",
    "                        #if convs[ses]['i'][0]=='56152':\n",
    "                        #    print(inp)\n",
    "                        #    print(ses)\n",
    "                        #    print(d['dialog'][0])\n",
    "                        #    print(convs[ses])\n",
    "                        convs[ses]['p']=d['personas']\n",
    "                convs[ses]['u'] += [[x['text'].strip(),x['id']] for x in d['dialog']]\n",
    "    for ses in convs:\n",
    "        all_uttrs+=[x[0] for x in convs[ses]['u']]\n",
    "    all_uttrs=pydash.uniq(all_uttrs)\n",
    "    tsv_groups=get_tsv_groups(tsvdiv,tsvgrouplen)\n",
    "    er1=0\n",
    "    er2=0\n",
    "    er3=0\n",
    "    er4=0\n",
    "    er5=0\n",
    "    contextlens=[]\n",
    "    data=[]\n",
    "    for tg in tsv_groups:\n",
    "        ses,ca=tg[0][:2]\n",
    "        if ses not in convs:\n",
    "            ses = f'{div}:{ses}'\n",
    "        if ses not in convs:\n",
    "            er2+=1\n",
    "            print('er2')\n",
    "            print(ses)\n",
    "            continue\n",
    "        conv=convs[ses]\n",
    "        if ca not in conv['i']:\n",
    "            #たまに異常な値がauthor名になっている\n",
    "            #print(conv['i'])\n",
    "            #print(ca)\n",
    "            #print(ses)\n",
    "            #print(conv)\n",
    "            er1+=1\n",
    "            continue\n",
    "        cind=conv['i'].index(ca)\n",
    "        rind=1-cind\n",
    "        ra=conv['i'][rind]\n",
    "        history=copy.copy(conv['u'])\n",
    "        if history[-1][1] != ra:\n",
    "            history=history[:-1]\n",
    "        if history[-1][1] != ra:\n",
    "            #たまに異常な値がauthor名になっている\n",
    "            er1+=1\n",
    "            continue\n",
    "        if history[-1][0]!=tg[0][2]:\n",
    "            #speaker1が2連続で喋ったりする\n",
    "            #print(ca)\n",
    "            #print(ra)\n",
    "            #print(history)\n",
    "            #print(tg[0])\n",
    "            #print(history[-1][0])\n",
    "            #print(tg[0][2])\n",
    "            er3+=1\n",
    "            continue\n",
    "        if history[-2][1] != ca:\n",
    "            er4+=1\n",
    "            continue\n",
    "        #historyを逆順にたどり、authorが交互になっている部分まで取る（ここまでの確認で最低2utteranceはある）\n",
    "        hind=0\n",
    "        context=[]\n",
    "        while True:\n",
    "            if history[-1-hind][1]!=ra or history[-2-hind][1]!=ca:\n",
    "                break\n",
    "            if len(context)>=30:\n",
    "                break\n",
    "            context.insert(0,history[-1-hind][0])\n",
    "            context.insert(0,history[-2-hind][0])\n",
    "        contextlens.append(len(context))\n",
    "        response=context[-1]\n",
    "        context=context[:-1]\n",
    "        response_cands=[x[2] for x in tg]\n",
    "        response_cands=add_response_cands(all_uttrs,response_cands,respcount-tsvgrouplen,[])\n",
    "        response_cands,label=randomize_cands(response_cands,0)\n",
    "        data.append([None,context,response,response_cands,label])\n",
    "    print(f'avgconlen:{sum(contextlens)/len(contextlens)}')\n",
    "    ud=[]\n",
    "    rd=[]\n",
    "    yd=[]\n",
    "    for persona,context,response,response_cands,label in tqdm(data,desc='write_data'):\n",
    "        convcon=[]\n",
    "        for c in context:\n",
    "            cc,seqover=conv_uttr(c,seqlen,vocab,unkkey)\n",
    "            if seqover:\n",
    "                er5+=1\n",
    "            assert(len(cc)==seqlen)\n",
    "            convcon.append(cc)\n",
    "        convcon=[*([[0] * seqlen] * (29 - len(convcon))), *convcon]\n",
    "        assert(len(convcon)==29)\n",
    "        for i in range(respcount):\n",
    "            res=response_cands[i]\n",
    "            yd.append(1 if res==response else 0)\n",
    "            ud.append(convcon)\n",
    "            convres,seqover=conv_uttr(res,seqlen,vocab,unkkey)\n",
    "            if seqover:\n",
    "                er5+=1\n",
    "            assert(len(convres)==seqlen)\n",
    "            rd.append(convres)\n",
    "        assert(sum(yd[-respcount:])==1)\n",
    "    print(f'er1:{er1},er2:{er2},er3:{er3},er4:{er4},er5:{er5}')\n",
    "    assert(len(ud)==len(rd))\n",
    "    assert(len(ud)==len(yd))\n",
    "    with open(outfile,'wb') as f:\n",
    "        pickle.dump([ud,rd,yd],f)\n",
    "#redditでは50でよかったが、mscはer5が多いので0になるまで増やす（trainはrandom性があるから0にならないかも）\n",
    "seqlen=200\n",
    "seqlen=50\n",
    "create_data(\n",
    "    'train',\n",
    "    'train',\n",
    "    2,\n",
    "    10,\n",
    "    seqlen,\n",
    "    'tm_train_msc_cemb.pkl'\n",
    ")\n",
    "create_data(\n",
    "    'test',\n",
    "    'test',\n",
    "    10,\n",
    "    10,\n",
    "    seqlen,\n",
    "    'tm_test_msc_cemb.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b90f8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp tm_test_msc_cemb.pkl dataset/reddit2/dev_reddit2.pkl\n",
    "!cp tm_test_msc_cemb.pkl dataset/reddit2/test_reddit2.pkl\n",
    "!cp tm_train_msc_cemb.pkl dataset/reddit2/train_reddit2.pkl\n",
    "!cp vocab_and_embeddings_msc.pkl dataset/reddit2/vocab_and_embeddings.pkl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.9.10-cikm2",
   "language": "python",
   "name": "venv-3.9.10-cikm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
